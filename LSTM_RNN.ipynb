{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_RNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1.**Problem Description**"
      ],
      "metadata": {
        "id": "LjIVDfAo-vsC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The problem that I will use to demonstrate sequence learning in this notebook is the **Movie review sentiment classification problem.** Each movie review is a sequence of words and the sentiment of each movie review must be classified. The dataset contains 50,000 movie reviews (good or bad). I split the dataset into two parts(each contains 25,000 reviews) for training and testing.The problem is to determine whether a given movie review has a positive or negative sentiment.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "258p0tNx_OBQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. **Dataset**"
      ],
      "metadata": {
        "id": "7Sl3gS5i-sGD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset that I used is **The Large Movie Review Dataset** (often referred to as the IMDB dataset). Keras provides access to the IMDB dataset built-in. And the **imdb.load_data()** function provides the dataset in a format that is ready for use in neural network and deep learning models. Each words in review have been replaced by integers that indicate the ordered frequency of each word in the dataset. The sentences in each review are Therefore comprised of a sequence of integers. "
      ],
      "metadata": {
        "id": "ArdVEFJzDRlV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. **Word Embedding**"
      ],
      "metadata": {
        "id": "jj0IQz7HFRZw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will map each movie review into a real vector domain, a popular technique when working with text called **word embedding**. This is a technique where words are encoded as real-valued vectors in a high dimensional space, where the similarity between words in terms of meaning translates to closeness in the vector space. I will map each word onto a 35 length real valued vector. I will also limit the total number of words that we are interested in modeling to the 6000 most frequent words, and zero out the rest. Finally, the sequence length (number of words) in each review varies, so I will constrain each review to be 600 words, truncating long reviews and pad the shorter reviews with zero values."
      ],
      "metadata": {
        "id": "lxcU7AlDFc76"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. **Importing useful classes and function**"
      ],
      "metadata": {
        "id": "3UCAGPPKHzCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(8)"
      ],
      "metadata": {
        "id": "QNqRvTGI-pjU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_words = 6000 # most frequent words\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASlM3N775Ta3",
        "outputId": "128eaec0-12fd-4824-be75-3e7e4ff8eba9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train.shape, X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZRxUcIXItU4",
        "outputId": "d59498de-009d-4b35-c956-3081b5410a3a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((25000,), (25000,))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the sample data."
      ],
      "metadata": {
        "id": "_T4h774-I3w3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train[1])\n",
        "print(\"The length of the 1st review is {}.\".format(len(X_train[1])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23BhYSkh54HQ",
        "outputId": "21c63901-d6ed-47f9-809b-f4b075072a56"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 194, 1153, 194, 2, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 2, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 2, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 2, 2, 349, 2637, 148, 605, 2, 2, 15, 123, 125, 68, 2, 2, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 2, 5, 2, 656, 245, 2350, 5, 4, 2, 131, 152, 491, 18, 2, 32, 2, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]\n",
            "The length of the 1st review is 189.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, make each and every review of same length.\n",
        "\n"
      ],
      "metadata": {
        "id": "xrkTCWDxJ6Js"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_review_length = 600\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
      ],
      "metadata": {
        "id": "FmPbfix56Db-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train[1])\n",
        "print(\"\\nThe length of the 1st review is {}.\".format(len(X_train[1])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhuGTm926jrl",
        "outputId": "34348cde-2941-4c16-8ae8-1cd98c1f6c62"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    1  194 1153  194    2   78  228    5    6\n",
            " 1463 4369 5012  134   26    4  715    8  118 1634   14  394   20   13\n",
            "  119  954  189  102    5  207  110 3103   21   14   69  188    8   30\n",
            "   23    7    4  249  126   93    4  114    9 2300 1523    5  647    4\n",
            "  116    9   35    2    4  229    9  340 1322    4  118    9    4  130\n",
            " 4901   19    4 1002    5   89   29  952   46   37    4  455    9   45\n",
            "   43   38 1543 1905  398    4 1649   26    2    5  163   11 3215    2\n",
            "    4 1153    9  194  775    7    2    2  349 2637  148  605    2    2\n",
            "   15  123  125   68    2    2   15  349  165 4362   98    5    4  228\n",
            "    9   43    2 1157   15  299  120    5  120  174   11  220  175  136\n",
            "   50    9 4373  228    2    5    2  656  245 2350    5    4    2  131\n",
            "  152  491   18    2   32    2 1212   14    9    6  371   78   22  625\n",
            "   64 1382    9    8  168  145   23    4 1690   15   16    4 1355    5\n",
            "   28    6   52  154  462   33   89   78  285   16  145   95]\n",
            "\n",
            "The length of the 1st review is 600.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. **LSTM model for sequence classification**"
      ],
      "metadata": {
        "id": "X-2G6TxTKlUA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first layer is the Embedded layer that uses 35 length vectors to represent each word. The next layer is the LSTM layer with 100 memory units (smart neurons). Finally, because this is a classification problem I use a Dense output layer with a single neuron and a sigmoid activation function to make 0 or 1 predictions for the two classes (good and bad) in the problem. Because it is a binary classification problem, log loss is used as the loss function."
      ],
      "metadata": {
        "id": "J61ZIc_zN-Sb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the model\n",
        "embedding_vecor_length = 35\n",
        "model = Sequential()\n",
        "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1NYPdsA7F4o",
        "outputId": "426200e6-ab77-424e-d8bf-5af28d8b4e67"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 600, 35)           210000    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 100)               54400     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 264,501\n",
            "Trainable params: 264,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "391/391 [==============================] - 42s 91ms/step - loss: 0.4777 - accuracy: 0.7620\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 35s 91ms/step - loss: 0.3107 - accuracy: 0.8711\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 35s 90ms/step - loss: 0.2402 - accuracy: 0.9072\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 35s 89ms/step - loss: 0.2037 - accuracy: 0.9212\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 35s 90ms/step - loss: 0.1861 - accuracy: 0.9278\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 35s 91ms/step - loss: 0.1614 - accuracy: 0.9400\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 35s 90ms/step - loss: 0.1745 - accuracy: 0.9332\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 36s 91ms/step - loss: 0.1186 - accuracy: 0.9571\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 35s 90ms/step - loss: 0.1187 - accuracy: 0.9573\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 35s 90ms/step - loss: 0.0825 - accuracy: 0.9713\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f597a6e0d10>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDaPR40i7jCq",
        "outputId": "a6f58993-c903-4f81-d7f9-f0a6d9f698cf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 85.04%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. **Using LSTM with Dropout**\n"
      ],
      "metadata": {
        "id": "txkqzOYj9YYn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because, Recurrent Neural networks like LSTM generally have the problem of overfitting.\n",
        "\n",
        "Dropout can be applied between layers using the Dropout Keras layer. We can do this just adding new Dropout layers between Embedding and LSTM layers and the LSTM and Dense output layers."
      ],
      "metadata": {
        "id": "0rxqpo5DO47Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "vFZBrJA78hc9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUnbEmT_9d2_",
        "outputId": "0249e959-5e9d-44cb-8fda-54e0f0903b58"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 600, 35)           210000    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 600, 35)           0         \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 100)               54400     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 264,501\n",
            "Trainable params: 264,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "391/391 [==============================] - 37s 90ms/step - loss: 0.4408 - accuracy: 0.7842\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 35s 89ms/step - loss: 0.2736 - accuracy: 0.8933\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 35s 89ms/step - loss: 0.2453 - accuracy: 0.9052\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 35s 89ms/step - loss: 0.2132 - accuracy: 0.9208\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 35s 90ms/step - loss: 0.1852 - accuracy: 0.9302\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 35s 90ms/step - loss: 0.1728 - accuracy: 0.9360\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 35s 89ms/step - loss: 0.1631 - accuracy: 0.9386\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 35s 89ms/step - loss: 0.1585 - accuracy: 0.9389\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 35s 89ms/step - loss: 0.1278 - accuracy: 0.9528\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 35s 89ms/step - loss: 0.1822 - accuracy: 0.9296\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f597148ea10>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score_with_drop =model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (score_with_drop[1]*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOioA_bO9wuG",
        "outputId": "d3fec082-7a7d-490f-9178-82798bf764bd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 85.66%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. **LSTM and CNN for sequence classification**"
      ],
      "metadata": {
        "id": "iS-SQXwL-99q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolutional neural networks excel at learning the spatial structure in input data.\n",
        "\n",
        "The data does have a one-dimensional spatial structure in the sequence of words in reviews and the CNN may be able to pick out invariant features for good and bad sentiment."
      ],
      "metadata": {
        "id": "SpPEBuXhSqz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
        "model.add(Conv1D(filters=35, kernel_size=3, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "wJKTGhVy_TDL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "model.fit(X_train, y_train, epochs=8, batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpbLE_MH_w7H",
        "outputId": "30124e58-fd32-4f09-ec7a-639d0a0962c2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 600, 35)           210000    \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 600, 35)           3710      \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPooling  (None, 300, 35)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 100)               54400     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 268,211\n",
            "Trainable params: 268,211\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/8\n",
            "391/391 [==============================] - 24s 55ms/step - loss: 0.4620 - accuracy: 0.7594\n",
            "Epoch 2/8\n",
            "391/391 [==============================] - 21s 55ms/step - loss: 0.2475 - accuracy: 0.9028\n",
            "Epoch 3/8\n",
            "391/391 [==============================] - 22s 55ms/step - loss: 0.1984 - accuracy: 0.9266\n",
            "Epoch 4/8\n",
            "391/391 [==============================] - 21s 55ms/step - loss: 0.1578 - accuracy: 0.9432\n",
            "Epoch 5/8\n",
            "391/391 [==============================] - 21s 55ms/step - loss: 0.1292 - accuracy: 0.9556\n",
            "Epoch 6/8\n",
            "391/391 [==============================] - 21s 55ms/step - loss: 0.1031 - accuracy: 0.9655\n",
            "Epoch 7/8\n",
            "391/391 [==============================] - 21s 55ms/step - loss: 0.0843 - accuracy: 0.9734\n",
            "Epoch 8/8\n",
            "391/391 [==============================] - 21s 55ms/step - loss: 0.0570 - accuracy: 0.9837\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f597adfa390>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score_res = model.evaluate(X_test,y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (score_res[1]*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQIpxWJU_4fg",
        "outputId": "5bd94ed5-9b4f-4e65-8ddd-77a8198cbbfd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 87.13%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. **LSTM with CNN and Dropout**"
      ],
      "metadata": {
        "id": "lofRxD0FW5U8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maybe adding the dropout layers increase the accuracy of the model."
      ],
      "metadata": {
        "id": "SN9fFkLBXCp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv1D(filters=35, kernel_size=3, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "XDlmLyWbAe46"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQRIr5ycXk0V",
        "outputId": "8c29f6d9-5d3a-4e2e-e486-0a26bc294e30"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (None, 600, 35)           210000    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 600, 35)           0         \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 600, 35)           3710      \n",
            "                                                                 \n",
            " max_pooling1d_2 (MaxPooling  (None, 300, 35)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (None, 100)               54400     \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 268,211\n",
            "Trainable params: 268,211\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "391/391 [==============================] - 24s 56ms/step - loss: 0.4428 - accuracy: 0.7796\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 22s 57ms/step - loss: 0.2530 - accuracy: 0.9002\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 22s 56ms/step - loss: 0.2021 - accuracy: 0.9233\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 21s 55ms/step - loss: 0.1692 - accuracy: 0.9376\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 21s 55ms/step - loss: 0.1459 - accuracy: 0.9493\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 22s 55ms/step - loss: 0.1250 - accuracy: 0.9563\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 22s 55ms/step - loss: 0.1044 - accuracy: 0.9644\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 22s 56ms/step - loss: 0.0893 - accuracy: 0.9706\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 22s 55ms/step - loss: 0.0769 - accuracy: 0.9750\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 22s 55ms/step - loss: 0.0703 - accuracy: 0.9774\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f597a4a2790>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score_cnn_with_drop = model.evaluate(X_test,y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (score_cnn_with_drop[1]*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5LmXK7KXqFs",
        "outputId": "99fb78c3-aff1-4089-c83b-ebe2b0e36647"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 87.27%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YZaznT-lYshl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}